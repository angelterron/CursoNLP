{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1: Importar las dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2: Pre Procesado de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    'data/train.csv'\n",
    "    , header = None\n",
    "    , names = cols\n",
    "    , engine = \"python\"\n",
    "    , encoding = \"latin1\"\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    'data/test.csv'\n",
    "    , header = None\n",
    "    , names = cols\n",
    "    , engine = \"python\"\n",
    "    , encoding = \"latin1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"date\", \"query\", \"user\"]\n",
    "         , axis = 1\n",
    "         , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Eliminamos la @ y su mención\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Eliminamos los links de las URL\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Nos quedamos solamente con los caracteres\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Eliminamos espacios en blanco adicionales\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \" i just received my G8 viola exam.. and its... well... .. disappointing.. :\\..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\../  \\../\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww that's a bummer. You shoulda got David C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no it's not behaving at all. i'm mad. why am ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0   Awww that's a bummer. You shoulda got David C...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0   I dived many times for the ball. Managed to s...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0   no it's not behaving at all. i'm mad. why am ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Va a crear el arreglo de tokens (palabras) y les va a asignar un valor\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    data_clean, target_vocab_size=2**16\n",
    ")\n",
    "\n",
    "# Se iteran las sentencias (tweets) y se tokenizan a arreglos para que la NN lo entienda\n",
    "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene el tweet con la mayor longitud de palabras\n",
    "MAX_LEN = max([len(sentence) for sentence in data_inputs])\n",
    "# Se hace el padding rellenando todas las frases con la máxima longitud\n",
    "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs\n",
    "                                                           , value = 0\n",
    "                                                           , padding = \"post\"\n",
    "                                                           , maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los primeros 800k son negativos y los segundos son negativos\n",
    "# Se obtienen de forma aleatoria primero los 800k y después\n",
    "# Se le suman 800k para tener los de la segunda parte\n",
    "test_idx = np.random.randint(0, 800000, 8000)\n",
    "test_idx = np.concatenate((test_idx, test_idx+800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se separan los conjuntos de datos, en labels no se ocupa axis para eliminar porque es un arreglo :)\n",
    "test_inputs = data_inputs[test_idx]\n",
    "test_labels = data_labels[test_idx]\n",
    "train_inputs = np.delete(data_inputs, test_idx, axis = 0)\n",
    "train_labels = np.delete(data_labels, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 3: Contrucción del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self\n",
    "                , vocab_size\n",
    "                , emb_dim = 128\n",
    "                , nb_filters = 50\n",
    "                , FFN_units = 512\n",
    "                , nb_classes = 2\n",
    "                , dropout_rate = 0.1\n",
    "                , training = False\n",
    "                , name = \"dcnn\"):\n",
    "        \n",
    "        super(DCNN, self).__init__(name = name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size\n",
    "                                         , emb_dim)\n",
    "        \n",
    "        self.bigram = layers.Conv1D(filters = nb_filters\n",
    "                                   , kernel_size = 2\n",
    "                                   , padding = \"valid\"\n",
    "                                   , activation = \"relu\")\n",
    "        self.trigram = layers.Conv1D(filters = nb_filters\n",
    "                                    , kernel_size = 3\n",
    "                                    , padding = \"valid\"\n",
    "                                    , activation = \"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters = nb_filters\n",
    "                                    , kernel_size = 4\n",
    "                                    , padding = \"valid\"\n",
    "                                    , activation = \"relu\")\n",
    "        \n",
    "        self.pool = layers.GlobalMaxPooling1D() #No tenemos variable de entrenamiento\n",
    "                                                #así que podemos usar la misma capa\n",
    "                                                #para cada paso de pooling\n",
    "        self.dense_1 = layers.Dense(units = FFN_units, activation = \"relu\")\n",
    "        \n",
    "        self.dropout = layers.Dropout(rate = dropout_rate)\n",
    "        \n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units = 1\n",
    "                                          , activation = \"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units = nb_classes\n",
    "                                          , activation = \"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis = -1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 5\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2 #len(set(train_labels))\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size = VOCAB_SIZE\n",
    "           , emb_dim = EMB_DIM\n",
    "           , nb_filters = NB_FILTERS\n",
    "           , FFN_units = FFN_UNITS\n",
    "           , nb_classes = NB_CLASSES\n",
    "           , dropout_rate = DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\"\n",
    "                , optimizer = \"adam\"\n",
    "                , metrics = [\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss = \"sparse_categorical_crossentropy\"\n",
    "                , optimizer = \"adam\"\n",
    "                , metrics = [\"sparse_categorical_accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn = Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Ultimo checkpoint restaurado!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 222ms/step - loss: 0.0020 - accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ckpt/ckpt-1'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(train_inputs[:1000]\n",
    "        , train_labels[:1000]\n",
    "        , batch_size = BATCH_SIZE\n",
    "        , epochs = NB_EPOCHS)\n",
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 4: Aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.2146038e-13]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn(np.array([tokenizer.encode(\" Awww that's a bummer. You shoulda got David Carr of Third Day to do it. D'\")]), training = False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Awww that's a bummer. You shoulda got David Carr of Third Day to do it. D\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
